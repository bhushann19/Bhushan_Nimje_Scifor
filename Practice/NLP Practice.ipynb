{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3c1f87",
   "metadata": {},
   "source": [
    "### Python String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa723ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n"
     ]
    }
   ],
   "source": [
    "a = \"Data\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096fe7d2",
   "metadata": {},
   "source": [
    "### String are arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb99c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "a = \"Dataset\"\n",
    "print(a[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74635bdc",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0bc9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a t\n",
      "a te\n",
      "t \n"
     ]
    }
   ],
   "source": [
    "# To get the output from position 3 to 6(not included)\n",
    "a = \"Data test\"\n",
    "print(a[3:6])\n",
    "\n",
    "# To get the output by negative indexing from -6 position to -2 position\n",
    "print(a[-6:-2])\n",
    "\n",
    "# Get the result from position 2 to 6 but you give result with increment of 2\n",
    "print(a[2:6:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919731b1",
   "metadata": {},
   "source": [
    "### String Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c221032b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Method:  ['Data', 'test']\n",
      "Lower Method:  data test\n",
      "Upper Method:  DATA TEST\n",
      "Replace Method:  Data t_st\n"
     ]
    }
   ],
   "source": [
    "print(\"Split Method: \", a.split())\n",
    "print(\"Lower Method: \", a.lower())\n",
    "print(\"Upper Method: \", a.upper())\n",
    "print(\"Replace Method: \", a.replace('e', '_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36156165",
   "metadata": {},
   "source": [
    "### String Concatination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51e4b920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data test train\n"
     ]
    }
   ],
   "source": [
    "a = \"Data test\"\n",
    "b = \" train\"\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26691ca",
   "metadata": {},
   "source": [
    "### Create a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d0b935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "file = open(\"dataset.txt\", \"w+\")\n",
    "\n",
    "# Step 2\n",
    "for i in range(5):\n",
    "    file.write(\"Line number is: %d\\r\\n\"%(i+1))\n",
    "    \n",
    "# Step 3\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194f967",
   "metadata": {},
   "source": [
    "### Append data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f2a0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "file = open(\"dataset.txt\", \"a+\")\n",
    "\n",
    "# Step 2\n",
    "for i in range(3):\n",
    "    file.write(\"Appending line number: %d\\r\\n\"%(i+1))\n",
    "\n",
    "# Step 3\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e9ee4",
   "metadata": {},
   "source": [
    "### Read the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ade2f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line number is: 1\n",
      "Line number is: 2\n",
      "Line number is: 3\n",
      "Line number is: 4\n",
      "Line number is: 5\n",
      "Appending line number: 1\n",
      "Appending line number: 2\n",
      "Appending line number: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "file = open(\"dataset.txt\", \"r\")\n",
    "\n",
    "# Step 2\n",
    "if file.mode == 'r':\n",
    "    content = file.read()\n",
    "    \n",
    "# Step 3\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044992e0",
   "metadata": {},
   "source": [
    "## Text PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53f257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29f0fc",
   "metadata": {},
   "source": [
    "### Text lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bdec49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather is too cloudy possibility of rain is high.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "input_str = \"Weather is too Cloudy possibility of Rain is High.\"\n",
    "lowercase_text(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d15bb",
   "metadata": {},
   "source": [
    "### Regex Expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a20435e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bought   candies from shop and   candies are in home.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Numbers\n",
    "def remove_num(text):\n",
    "    result = re.sub('\\d+', \" \", text)\n",
    "    return result\n",
    "    \n",
    "input_str = \"You bought 6 candies from shop and 4 candies are in home.\"\n",
    "remove_num(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f97c6",
   "metadata": {},
   "source": [
    "### inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a072299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You bought six candies from shop, and four candies are in home.\n"
     ]
    }
   ],
   "source": [
    "import inflect\n",
    "\n",
    "# Initialize the inflect engine\n",
    "p = inflect.engine()\n",
    "\n",
    "# Convert number into text\n",
    "def convert_num(text):\n",
    "    # Split strings into list of texts\n",
    "    temp_string = text.split()\n",
    "\n",
    "    # Initialize empty list\n",
    "    new_str = []\n",
    "\n",
    "    for word in temp_string:\n",
    "        # If text is a digit, convert the digit to words and append into the new str list\n",
    "        if word.isdigit():\n",
    "            new_str.append(p.number_to_words(word))\n",
    "        # Append the texts as it is\n",
    "        else:\n",
    "            new_str.append(word)\n",
    "\n",
    "    # Join the texts of new str to form a string\n",
    "    temp_str = ' '.join(new_str)\n",
    "\n",
    "    return temp_str\n",
    "\n",
    "input_str = 'You bought 6 candies from shop, and 4 candies are in home.'\n",
    "print(convert_num(input_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d8610",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7535d125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Are you excited After a week we will be in Shimla'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "input_str = \"Hey, Are you excited?, After a week, we will be in Shimla!!!\"\n",
    "remove_punct(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70913865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282cf92",
   "metadata": {},
   "source": [
    "### Remove Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e32f2e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data', 'new', 'oil', 'A.I', 'last', 'invention']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Remove Stopwords \n",
    "def rem_stopwords(text):\n",
    "    stop_words = set(stopwords.words('English'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in string.punctuation]\n",
    "    filtered_text = [word for word in filtered_text if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "ex_text = \"Data is the new oil. A.I is the last invention.\"\n",
    "rem_stopwords(ex_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d961920",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12c17a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'is',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolut',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individu',\n",
       " 'would',\n",
       " 'gener',\n",
       " 'terabyt',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "#stem words in the list of tokenised words\n",
    "def stem_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced788c3",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4913bb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bhushannimje/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'be', 'the', 'new', 'revolution', 'in', 'the', 'World', ',', 'in', 'a', 'day', 'one', 'individual', 'would', 'generate', 'terabytes', 'of', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    # Tokenize the text\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # Lemmatize each word\n",
    "    lemmas = [wordnet.WordNetLemmatizer().lemmatize(word, pos='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "text = \"Data is the new revolution in the World, in a day one individual would generate terabytes of data.\"\n",
    "lemmatized_words = lemmatize_word(text)\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223522f6",
   "metadata": {},
   "source": [
    "### Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ce8e830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Are', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('afraid', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# convert text into word tokens with their tags\n",
    "def pos_tagg(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return pos_tag(word_tokens)\n",
    "\n",
    "pos_tagg(\"Are you afraid of something?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733b333",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f2a6b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ red/JJ parrot/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# here we define chunking function with text and regular\n",
    "# expressions representing grammar as parameter\n",
    "def chunking(text, grammar):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    # label words with pos\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "    \n",
    "    # create chunk parser using grammar\n",
    "    chunkParser = nltk.RegexpParser(grammar)\n",
    "    \n",
    "    # test it on the list of word tokens with tagged pos\n",
    "    tree = chunkParser.parse(word_pos)\n",
    "    \n",
    "    # iterate over the parse tree and print subtrees\n",
    "    for subtree in tree.subtrees():\n",
    "        print(subtree)\n",
    "        \n",
    "# sentence to be chunked\n",
    "sentence = 'the little red parrot is flying in the sky'\n",
    "\n",
    "# Regular expression grammar for Noun Phrase (NP)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "chunking(sentence, grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6053292",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36f2359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Brain/NNP)\n",
      "  (PERSON Lara/NNP)\n",
      "  scored/VBD\n",
      "  the/DT\n",
      "  highest/JJS\n",
      "  400/CD\n",
      "  runs/NNS\n",
      "  in/IN\n",
      "  a/DT\n",
      "  test/NN\n",
      "  match/NN\n",
      "  which/WDT\n",
      "  played/VBD\n",
      "  in/IN\n",
      "  between/IN\n",
      "  (ORGANIZATION WI/NNP)\n",
      "  and/CC\n",
      "  (GPE England/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "def ner(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    #pos tagging of words\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "    \n",
    "    #tree of word entities\n",
    "    print(ne_chunk(word_pos))\n",
    "    \n",
    "text = 'Brain Lara scored the highest 400 runs in a test match which played in between WI and England.'\n",
    "ner(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b391693d",
   "metadata": {},
   "source": [
    "### Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c7cf907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"dataset, Data is a new fuel\"\n",
    "r2 = re.findall(r\"^\\w+\", text)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6a8f3",
   "metadata": {},
   "source": [
    "### re.split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53d87337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset', 'Data', 'is', 'a', 'new', 'fuel']\n"
     ]
    }
   ],
   "source": [
    "text = \"dataset, Data is a new fuel\"\n",
    "r2 = re.findall(r\"\\w+\", text)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "413eff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset,', 'Data', 'is', 'a', 'new', 'fuel']\n"
     ]
    }
   ],
   "source": [
    "r2 = re.split(r\"\\s\", text)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7924bf26",
   "metadata": {},
   "source": [
    "## RegEx methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cedb0f2",
   "metadata": {},
   "source": [
    "### re.match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86a418dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('icecream', 'images')\n"
     ]
    }
   ],
   "source": [
    "lists = ['icecream images', 'i immitated', 'inner peace']\n",
    "\n",
    "for i in lists:\n",
    "    a = re.match(\"(i\\w+)\\W(i\\w+)\", i)\n",
    "    \n",
    "    if a:\n",
    "        print(a.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6b757",
   "metadata": {},
   "source": [
    "### Finding Pattern in the text(re.search())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8157cd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are looking for 'playing' in 'Raju is playing outside.'Found match\n",
      "You are looking for 'dataset' in 'Raju is playing outside.'No match found\n"
     ]
    }
   ],
   "source": [
    "pattern = [\"playing\", \"dataset\"]\n",
    "text = \"Raju is playing outside.\"\n",
    "\n",
    "for p in pattern:\n",
    "    print(\"You are looking for '%s' in '%s'\" % (p, text), end='')\n",
    "    \n",
    "    if re.search(p, text):\n",
    "        print(\"Found match\")\n",
    "    else:\n",
    "        print(\"No match found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf33278",
   "metadata": {},
   "source": [
    "### Using re.findall() for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03c9b4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XYX@gmail.com\n",
      " lmn@gmail.com\n",
      " efg@gmail.com\n"
     ]
    }
   ],
   "source": [
    "email = \"Abc@gmail.com, XYX@gmail.com, lmn@gmail.com, efg@gmail.com\"\n",
    "\n",
    "find_email = re.findall(r' [\\w\\.]+@[\\w\\.]+', email)\n",
    "for i in find_email:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be984c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
