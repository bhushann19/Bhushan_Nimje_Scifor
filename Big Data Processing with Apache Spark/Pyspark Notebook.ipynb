{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b4e56df-efbc-41d5-bb18-905ac8118c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf5fe5-0c87-4aa2-ae56-4a8538e2dde4",
   "metadata": {},
   "source": [
    "### from pyspark.sql import SparkSession\n",
    "\n",
    "What It Is: This imports the SparkSession class from the pyspark.sql module.\n",
    "\n",
    "\n",
    "\n",
    "Purpose: SparkSession is the entry point for using Spark functionality. It allows you to create DataFrames, execute SQL queries, and access other Spark functionalities. It replaces the older SparkContext and SQLContext in newer versions of PySpark.\n",
    "\n",
    "\n",
    "### from pyspark.sql import functions as F\n",
    "\n",
    "What It Is: This imports the functions module from pyspark.sql and gives it the alias F.\n",
    "\n",
    "\n",
    "\n",
    "Purpose: The functions module contains many built-in functions for performing operations on DataFrames, such as F.col(), \n",
    "\n",
    "#### F.lit(), F.concat(), and others. Using the alias F makes the code cleaner and shorter.\n",
    "\n",
    "    F.lit(value)\n",
    "\n",
    "        What It Is: Creates a column containing a constant value.\n",
    "\n",
    "        \n",
    "        Purpose: Used to add a fixed value as a new column or in operations with other columns.\n",
    "            \n",
    "    \n",
    "    F.col(column_name)\n",
    "\n",
    "        What It Is: Refers to an existing column in a DataFrame.\n",
    "\n",
    "\n",
    "        Purpose: Allows you to perform operations or transformations on that specific column.\n",
    "\n",
    "\n",
    "    F.concat(*cols)\n",
    "\n",
    "        What It Is: Combines multiple columns into a single column of strings.\n",
    "\n",
    "\n",
    "        Purpose: Used to merge values from different columns into one column, typically for concatenating text data.\n",
    "\n",
    "### from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
    "\n",
    "What It Is: This imports data type classes and schema definition classes from pyspark.sql.types.\n",
    "\n",
    "\n",
    "Purpose:\n",
    "\n",
    "\n",
    "IntegerType: Defines a column of integer type in a DataFrame schema.\n",
    "\n",
    "\n",
    "StringType: Defines a column of string type in a DataFrame schema.\n",
    "\n",
    "\n",
    "StructType: Represents a schema for a DataFrame, where you can define the structure of the DataFrame by specifying the types of each column.\n",
    "\n",
    "\n",
    "StructField: Defines a single field (column) in a StructType, including its name and type.\n",
    "### from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "\n",
    "What It Is: This imports classes from the pyspark.ml.feature module.\n",
    "Purpose:\n",
    "\n",
    "\n",
    "VectorAssembler: Combines multiple columns into a single vector column, which is often required for machine learning algorithms that expect features in a vector format.\n",
    "\n",
    "\n",
    "StringIndexer: Converts categorical string columns into numerical indices, which is useful for algorithms that require numeric inputs.\n",
    "### from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "What It Is: This imports the RandomForestClassifier class from the pyspark.ml.classification module.\n",
    "\n",
    "\n",
    "Purpose: RandomForestClassifier is a machine learning algorithm for classification tasks. It builds multiple decision trees and combines their results to improve accuracy and control over-fitting.\n",
    "### from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "What It Is: This imports the MulticlassClassificationEvaluator class from the pyspark.ml.evaluation module.\n",
    "\n",
    "\n",
    "Purpose: MulticlassClassificationEvaluator is used to evaluate the performance of a multiclass classification model. It provides metrics such as accuracy, precision, recall, and F1 score.\n",
    "### import random\n",
    "\n",
    "What It Is: This imports the random module from the Python standard library.\n",
    "\n",
    "\n",
    "Purpose: The random module provides functions to generate random numbers, perform random selections, and shuffle sequences. It is often used for tasks such as data sampling, splitting datasets, or adding randomness to experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410888e4-f32f-4772-a9f7-9cb6ad8d4e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7a126-c5c2-4c28-ad29-3d274141644b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16b0e2cd-b4ab-4f7e-a46a-358f6ebb7b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-----+-----+\n",
      "| Name|Age|Country|Score|Label|\n",
      "+-----+---+-------+-----+-----+\n",
      "|  Bob| 35| France|   87|    0|\n",
      "|  Bob| 18|  India|   82|    0|\n",
      "|Alice| 30|  Japan|   72|    0|\n",
      "|Grace| 52|  India|   79|    0|\n",
      "|David| 30|  China|   88|    0|\n",
      "+-----+---+-------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "# This initializes a Spark application with the name \"PySpark Example\"\n",
    "spark = SparkSession.builder.appName(\"PySpark Example\").getOrCreate()\n",
    "\n",
    "# Define a schema for the DataFrame\n",
    "# The schema specifies the column names and data types for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),       # Column for names (strings), nullable\n",
    "    StructField(\"Age\", IntegerType(), True),        # Column for age (integers), nullable\n",
    "    StructField(\"Country\", StringType(), True),     # Column for country (strings), nullable\n",
    "    StructField(\"Score\", IntegerType(), True),      # Column for score (integers), nullable\n",
    "    StructField(\"Label\", IntegerType(), True)       # Column for label (integers), nullable (used for classification)\n",
    "])\n",
    "\n",
    "# Generate a large dataset (100,000 rows) with random data\n",
    "# The data consists of random values for each of the columns defined in the schema\n",
    "names = [\"Alice\", \"Bob\", \"Cathy\", \"David\", \"Eva\", \"Frank\", \"Grace\", \"Helen\"]  # Possible names\n",
    "countries = [\"USA\", \"Canada\", \"UK\", \"Germany\", \"France\", \"India\", \"China\", \"Japan\"]  # Possible countries\n",
    "\n",
    "# Create a list of tuples with random data\n",
    "# Each tuple represents a row in the dataset\n",
    "data = [(random.choice(names),                         # Random name\n",
    "         random.randint(18, 60),                      # Random age between 18 and 60\n",
    "         random.choice(countries),                    # Random country\n",
    "         random.randint(50, 100),                     # Random score between 50 and 100\n",
    "         1 if random.randint(50, 100) > 80 else 0)    # Label: 1 if score > 80, otherwise 0\n",
    "        for _ in range(100000)]                        # Generate 100,000 such tuples\n",
    "\n",
    "# Create a DataFrame with the generated data and defined schema\n",
    "# This converts the list of tuples into a Spark DataFrame using the schema\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the first 5 rows of the DataFrame\n",
    "# This displays a sample of the data to the console for verification\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe2b9e1-9eb3-48d5-a8c8-a1a30fd495b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=================================================>       (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get the number of rows in the DataFrame\n",
    "row_count = df.count()\n",
    "print(f\"Number of rows: {row_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "036d32ac-2c6e-4eb2-a8e7-af937588d64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "| Name|Country|Score|\n",
      "+-----+-------+-----+\n",
      "|Frank|     UK|   81|\n",
      "|Cathy|  China|   90|\n",
      "|David| Canada|   93|\n",
      "|Frank| Canada|   87|\n",
      "|Frank|     UK|   82|\n",
      "+-----+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns\n",
    "selected_df = df.select(\"Name\", \"Country\", \"Score\")\n",
    "selected_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ab4874-857a-4db6-90f0-c689bcb1f3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-----+-----+\n",
      "| Name|Age|Country|Score|Label|\n",
      "+-----+---+-------+-----+-----+\n",
      "|Frank| 55|     UK|   81|    1|\n",
      "|Cathy| 51|  China|   90|    0|\n",
      "|David| 59| Canada|   93|    0|\n",
      "|Frank| 32| Canada|   87|    0|\n",
      "|Frank| 32|     UK|   82|    0|\n",
      "+-----+---+-------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame where Score > 80\n",
    "filtered_df = df.filter(df[\"Score\"] > 80)\n",
    "filtered_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4253a710-0307-4322-9bf8-ac5adf3bbad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===>                                                     (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Country|    Average_Score|\n",
      "+-------+-----------------+\n",
      "|Germany|74.91144501278772|\n",
      "| France|74.85205479452055|\n",
      "|  China|74.86080643888582|\n",
      "|  India|74.89501438159157|\n",
      "|    USA|75.02417353718081|\n",
      "|     UK|75.24174222797927|\n",
      "| Canada|75.14404476418865|\n",
      "|  Japan|75.16355513004628|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Group by \"Country\" column and calculate the average of the \"Score\" column\n",
    "grouped_df = df.groupBy(\"Country\").agg(F.avg(\"Score\").alias(\"Average_Score\"))\n",
    "\n",
    "# Show the resulting DataFrame with the average scores for each country\n",
    "grouped_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e45553-7ba3-4993-859c-d60213fe3b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-----+-----+---------+\n",
      "| Name|Age|Country|Score|Label|New_Score|\n",
      "+-----+---+-------+-----+-----+---------+\n",
      "|Frank| 55|     UK|   81|    1|       91|\n",
      "|Cathy| 51|  China|   90|    0|      100|\n",
      "|David| 59| Canada|   93|    0|      103|\n",
      "|Frank| 32| Canada|   87|    0|       97|\n",
      "|Frank| 32|     UK|   82|    0|       92|\n",
      "+-----+---+-------+-----+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column named 'New_Score' to the DataFrame\n",
    "# The value of 'New_Score' is calculated by adding 10 to the existing 'Score' column\n",
    "transformed_df = df.withColumn(\"New_Score\", df[\"Score\"] + 10)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame with the new column added\n",
    "transformed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5279361-28aa-4f45-93ec-05cf1db0a7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-----+-----+\n",
      "| Name|Age|Country|Score|Label|\n",
      "+-----+---+-------+-----+-----+\n",
      "|  Bob| 35|     UK|  100|    0|\n",
      "|Cathy| 37|  Japan|  100|    0|\n",
      "|Frank| 32|  China|  100|    1|\n",
      "|David| 24|     UK|  100|    0|\n",
      "|  Eva| 57|  China|  100|    0|\n",
      "+-----+---+-------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame by the 'Score' column in descending order\n",
    "# The 'orderBy' function is used to sort the DataFrame based on the specified column(s)\n",
    "# 'df[\"Score\"].desc()' indicates that the sorting should be in descending order\n",
    "sorted_df = df.orderBy(df[\"Score\"].desc())\n",
    "\n",
    "# Display the first 5 rows of the sorted DataFrame\n",
    "# This shows the top 5 rows after sorting, with the highest 'Score' values appearing first\n",
    "sorted_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38cefdab-1573-4071-91eb-41629591936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-----+-----+\n",
      "| Name|Age|Country|Score|Label|\n",
      "+-----+---+-------+-----+-----+\n",
      "|Frank| 55|     UK|   81|    1|\n",
      "|Cathy| 51|  China|   90|    0|\n",
      "|David| 59| Canada|   93|    0|\n",
      "|Frank| 32| Canada|   87|    0|\n",
      "|Frank| 32|     UK|   82|    0|\n",
      "+-----+---+-------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop rows with missing values (if any)\n",
    "cleaned_df = df.dropna()\n",
    "cleaned_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aa0cc54-22ea-44b6-aded-577f5ee22e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "cleaned_df.write.csv(\"output_dataset.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d997cca-b9a9-4558-95c7-edc0b8750e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Prepare the data for machine learning by converting categorical values to numeric values\n",
    "\n",
    "# Create a StringIndexer object\n",
    "# This will map categorical values in the 'Country' column to numeric indices\n",
    "indexer = StringIndexer(inputCol=\"Country\", outputCol=\"CountryIndex\")\n",
    "\n",
    "# Fit the StringIndexer model to the DataFrame and transform the data\n",
    "# The 'fit' method learns the mapping from categorical values to numeric indices\n",
    "# The 'transform' method applies this mapping to create a new column 'CountryIndex' in the DataFrame\n",
    "df_indexed = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03826ea4-98d1-4b46-b8dc-3474585df2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VectorAssembler\n",
    "# inputCols: List of columns to combine into one vector\n",
    "# outputCol: Name of the new column that will store the feature vectors\n",
    "assembler = VectorAssembler(inputCols=[\"Age\", \"CountryIndex\", \"Score\"], outputCol=\"features\")\n",
    "\n",
    "# Transform the DataFrame\n",
    "# The transform method combines the 'Age', 'CountryIndex', and 'Score' columns into a single vector column named 'features'\n",
    "df_features = assembler.transform(df)\n",
    "\n",
    "# Display the resulting DataFrame to see the new 'features' column\n",
    "df_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dab2c64-0b0e-4ecf-9e6a-8d1de727137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in DataFrame: ['Name', 'Age', 'Country', 'Score', 'Label', 'CountryIndex', 'features']\n"
     ]
    }
   ],
   "source": [
    "# Check if 'Label' column exists in df_features\n",
    "print(\"Columns in DataFrame:\", df_features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5df95ba-ccfc-4a3b-916e-e93796cf6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and test sets\n",
    "(training_data, test_data) = df_features.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc942564-b658-4992-9987-4c58a5a0c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"Label\", featuresCol=\"features\", numTrees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca32da0e-ac7b-4951-92d6-a9386735de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = rf.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1c2681e-cfbb-4592-bbd2-272c45cfc5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aed1927a-ac7f-487d-8a45-a349247ce97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:================================>                        (9 + 7) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6042640921204807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62717224-98a7-424e-a619-ec2faff82a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-----+-----+----------+\n",
      "| Name|Age|Country|Score|Label|prediction|\n",
      "+-----+---+-------+-----+-----+----------+\n",
      "|Alice| 18| Canada|   69|    1|       0.0|\n",
      "|Alice| 18| France|   59|    1|       0.0|\n",
      "|Alice| 19|  India|   64|    1|       0.0|\n",
      "|Alice| 19|  Japan|   62|    1|       0.0|\n",
      "|Alice| 19|  Japan|   80|    0|       0.0|\n",
      "+-----+---+-------+-----+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show some prediction results\n",
    "predictions.select(\"Name\", \"Age\", \"Country\", \"Score\", \"Label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c3510-7458-45d7-9d67-615407aa4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
