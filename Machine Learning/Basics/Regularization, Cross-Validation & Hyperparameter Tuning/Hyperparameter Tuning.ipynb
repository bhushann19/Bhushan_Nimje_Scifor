{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170fc2ea",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning model to improve its performance. Hyperparameters are configuration settings that are external to the model and cannot be directly learned from the data. Examples of hyperparameters include the learning rate in gradient descent, the number of hidden layers in a neural network, and the regularization strength in linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd01d72",
   "metadata": {},
   "source": [
    "### Importance of Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c485b",
   "metadata": {},
   "source": [
    "Optimizing hyperparameters is essential because:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e27a7c",
   "metadata": {},
   "source": [
    "1. Improves Model Performance: Properly tuned hyperparameters can significantly enhance a model's performance on unseen data.\n",
    "2. Prevents Overfitting: Tuning hyperparameters helps in mitigating overfitting and underfitting, leading to models that generalize well.\n",
    "3. Enhances Robustness: Models with tuned hyperparameters are more robust and reliable, making them suitable for deployment in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f6684",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d08468",
   "metadata": {},
   "source": [
    "1. Grid Search\n",
    "2. Random Search\n",
    "3. Bayesian Optimization\n",
    "4. Gradient-based Optimization\n",
    "5. Manual Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6041e",
   "metadata": {},
   "source": [
    "### 1. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a5194",
   "metadata": {},
   "source": [
    "Grid search is a brute-force approach that systematically evaluates the model's performance across a predefined grid of hyperparameters. For each combination of hyperparameters, the model is trained and evaluated using cross-validation. The combination with the best performance is selected as the optimal set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da1e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize a random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Initialize a grid search object\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef21a43",
   "metadata": {},
   "source": [
    "#### Code Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adcd731",
   "metadata": {},
   "source": [
    "1. Load Data: We load the iris dataset, which contains features (X) and target labels (y).\n",
    "2. Define Hyperparameters Grid: We define a grid of hyperparameters to search over.\n",
    "3. Initialize Model: We initialize a random forest classifier.\n",
    "4. Initialize Grid Search: We initialize a grid search object with the random forest classifier and the hyperparameters grid.\n",
    "5. Perform Grid Search: We perform grid search using cross-validation to find the best set of hyperparameters.\n",
    "6. Print Results: We print the best hyperparameters found by the grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e778d",
   "metadata": {},
   "source": [
    "### 2. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62919cab",
   "metadata": {},
   "source": [
    "Random search randomly samples hyperparameters from predefined distributions and evaluates the model's performance for each random draw. This method is more efficient than grid search and often yields comparable or better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3331bff",
   "metadata": {},
   "source": [
    "#### Steps for Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5230091f",
   "metadata": {},
   "source": [
    "1. Define the Hyperparameter Space: Specify the range or distribution for each hyperparameter to be tuned.\n",
    "2. Randomly Sample Hyperparameters: Randomly select combinations of hyperparameters from the defined space.\n",
    "3. Evaluate Model Performance: Train and evaluate the model using each combination of hyperparameters.\n",
    "4. Select the Best Model: Choose the model with the best performance based on a chosen evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9384ef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 15, 'max_depth': None, 'bootstrap': True}\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 500, 50),  # Number of trees in the forest\n",
    "    'max_depth': [None] + list(np.arange(5, 30, 5)),  # Maximum depth of the tree\n",
    "    'min_samples_split': np.arange(2, 20, 2),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': np.arange(1, 20, 2),  # Minimum number of samples required to be at a leaf node\n",
    "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Initialize a random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Initialize a random search object\n",
    "random_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Perform random search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eee360",
   "metadata": {},
   "source": [
    "#### Code Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3e10d",
   "metadata": {},
   "source": [
    "1. Load Data: We load the iris dataset, which contains features (X) and target labels (y).\n",
    "2. Split Data: We split the data into training and testing sets.\n",
    "3. Define Hyperparameter Space: We define the hyperparameter space for the random forest classifier, specifying the range or distribution for each hyperparameter.\n",
    "4. Initialize Model: We initialize a random forest classifier.\n",
    "5. Initialize Random Search: We initialize a RandomizedSearchCV object with the random forest classifier, the hyperparameter space, and other parameters such as the number of iterations (n_iter), cross-validation folds (cv), and scoring metric (scoring).\n",
    "6. Perform Random Search: We perform random search using the fit method, which evaluates the performance of different hyperparameter combinations using cross-validation.\n",
    "7. Print Best Hyperparameters: We print the best hyperparameters found during random search.\n",
    "8. Evaluate on Test Set: We evaluate the best model found during random search on the test set to obtain its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530067bc",
   "metadata": {},
   "source": [
    "### 3. Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554210c4",
   "metadata": {},
   "source": [
    "Bayesian optimization uses probabilistic models to select hyperparameters based on past evaluations. It models the objective function and updates its belief about the hyperparameter space iteratively, focusing on promising regions to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175e8034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameters: [-1.5586853619654595, -3.204702953080796]\n",
      "Minimum Value: -1.9979358691053162\n"
     ]
    }
   ],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from skopt.utils import use_named_args\n",
    "import numpy as np\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x):\n",
    "    # Example objective function (2D)\n",
    "    return np.sin(x[0]) + np.cos(x[1])\n",
    "\n",
    "# Define the search space\n",
    "space = [Real(-5, 5, name='x1'), Real(-5, 5, name='x2')]\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "result = gp_minimize(objective_function, space, n_calls=20, random_state=42)\n",
    "\n",
    "# Print the optimal parameters and minimum value\n",
    "print(\"Optimal Parameters:\", result.x)\n",
    "print(\"Minimum Value:\", result.fun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96868ab2",
   "metadata": {},
   "source": [
    "#### Code Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da1c43",
   "metadata": {},
   "source": [
    "1. Objective Function: We define a simple objective function objective_function(x) that takes a vector x as input and returns the value of the function. In this example, we use a 2D function sin(x[0]) + cos(x[1]), but any black-box function can be used.\n",
    "\n",
    "2. Search Space: We define the search space using skopt.space.Real to specify the range of each parameter (x1 and x2) that Bayesian Optimization will explore. Here, we define each parameter to be in the range [-5, 5].\n",
    "\n",
    "3. Perform Bayesian Optimization: We use gp_minimize from scikit-optimize to perform Bayesian Optimization. We pass the objective function, search space, and the number of calls (n_calls) as parameters. n_calls specifies the total number of evaluations of the objective function.\n",
    "\n",
    "4. Print Results: We print the optimal parameters (result.x) and the minimum value (result.fun) found by Bayesian Optimization.\n",
    "\n",
    "5. gp_minimize is used to perform the optimization. It stands for Gaussian Process Minimization, where a probabilistic surrogate model (Gaussian Process) is used to model the objective function.\n",
    "\n",
    "6. The space variable defines the search space for the optimization problem. Here, we use the Real class to define continuous variables (x1 and x2) with specified ranges.\n",
    "\n",
    "7. The objective_function is the black-box function we aim to optimize. In this example, it's a simple 2D function, but it could be any complex function (e.g., hyperparameters tuning for machine learning models).\n",
    "\n",
    "8. The n_calls parameter specifies the total number of evaluations of the objective function. Increasing n_calls may lead to better optimization results but will also increase computation time.\n",
    "\n",
    "9. Bayesian Optimization automatically balances exploration and exploitation by iteratively updating a probabilistic model of the objective function and selecting new points to evaluate based on an acquisition function (e.g., expected improvement, probability of improvement).\n",
    "\n",
    "10. The result of Bayesian Optimization is a set of optimal parameters that minimize (or maximize) the objective function, along with the corresponding minimum (or maximum) value of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d070c4",
   "metadata": {},
   "source": [
    "### 4. Gradient-based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49345c6",
   "metadata": {},
   "source": [
    "Gradient-based optimization treats hyperparameter tuning as an optimization problem. It computes the gradient of the objective function with respect to the hyperparameters and adjusts them iteratively to minimize/maximize the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a493f28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Objective = 43.25671257927961\n",
      "Iteration 100: Objective = 1.9091300996378586\n",
      "Iteration 200: Objective = 1.2590897678136568\n",
      "Iteration 300: Objective = 1.046447857129863\n",
      "Iteration 400: Objective = 0.9658948334678626\n",
      "Iteration 500: Objective = 0.9289791102857707\n",
      "Iteration 600: Objective = 0.9086146871000866\n",
      "Iteration 700: Objective = 0.8957839270183312\n",
      "Iteration 800: Objective = 0.8870692826429796\n",
      "Iteration 900: Objective = 0.8809283939706222\n",
      "Optimized Model Parameters: [0.28531068 1.72607585 2.876489   4.00389594]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the objective function (e.g., mean squared error)\n",
    "def objective_function(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error between the predictions and the actual values.\n",
    "    \n",
    "    Args:\n",
    "    - theta: Model parameters\n",
    "    - X: Input features\n",
    "    - y: Target labels\n",
    "    \n",
    "    Returns:\n",
    "    - The mean squared error\n",
    "    \"\"\"\n",
    "    predictions = np.dot(X, theta)  # Compute predictions\n",
    "    error = predictions - y  # Compute error\n",
    "    mse = np.mean(error ** 2)  # Compute mean squared error\n",
    "    return mse\n",
    "\n",
    "# Define the gradient of the objective function\n",
    "def gradient(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the mean squared error with respect to the model parameters.\n",
    "    \n",
    "    Args:\n",
    "    - theta: Model parameters\n",
    "    - X: Input features\n",
    "    - y: Target labels\n",
    "    \n",
    "    Returns:\n",
    "    - The gradient of the mean squared error\n",
    "    \"\"\"\n",
    "    predictions = np.dot(X, theta)  # Compute predictions\n",
    "    error = predictions - y  # Compute error\n",
    "    gradient = 2 * np.dot(X.T, error) / X.shape[0]  # Compute gradient (average over samples)\n",
    "    return gradient\n",
    "\n",
    "# Define gradient descent optimization algorithm\n",
    "def gradient_descent(X, y, learning_rate=0.01, num_iterations=1000):\n",
    "    \"\"\"\n",
    "    Perform gradient descent optimization to minimize the objective function.\n",
    "    \n",
    "    Args:\n",
    "    - X: Input features\n",
    "    - y: Target labels\n",
    "    - learning_rate: Learning rate (step size)\n",
    "    - num_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    - The optimized model parameters\n",
    "    \"\"\"\n",
    "    # Initialize model parameters randomly\n",
    "    theta = np.random.randn(X.shape[1])\n",
    "    \n",
    "    # Perform gradient descent iterations\n",
    "    for i in range(num_iterations):\n",
    "        # Compute gradient of the objective function\n",
    "        grad = gradient(theta, X, y)\n",
    "        \n",
    "        # Update model parameters in the opposite direction of the gradient\n",
    "        theta -= learning_rate * grad\n",
    "        \n",
    "        # Print progress every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Objective = {objective_function(theta, X, y)}\")\n",
    "    \n",
    "    return theta\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 3)  # 100 samples, 3 features\n",
    "theta_true = np.array([2, 3, 4])  # True model parameters\n",
    "y = np.dot(X, theta_true) + np.random.randn(100)  # Target labels with noise\n",
    "\n",
    "# Add a column of ones to X for the intercept term\n",
    "X_with_intercept = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "# Perform gradient descent optimization\n",
    "theta_optimized = gradient_descent(X_with_intercept, y)\n",
    "\n",
    "# Print the optimized model parameters\n",
    "print(\"Optimized Model Parameters:\", theta_optimized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da36e91",
   "metadata": {},
   "source": [
    "#### Code Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d66c693",
   "metadata": {},
   "source": [
    "1. Objective Function: The objective_function computes the mean squared error between the predictions and the actual values.\n",
    "2. Gradient: The gradient function computes the gradient of the mean squared error with respect to the model parameters using the chain rule.\n",
    "3. Gradient Descent: The gradient_descent function performs gradient descent optimization to minimize the objective function. It initializes the model parameters randomly and iteratively updates them in the opposite direction of the gradient.\n",
    "4. Sample Data: We generate some sample data with features (X) and target labels (y) with noise.\n",
    "5. Intercept Term: We add a column of ones to X for the intercept term.\n",
    "6. Optimization: We use gradient descent to optimize the model parameters (theta_optimized).\n",
    "7. Results: We print the optimized model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d92b07",
   "metadata": {},
   "source": [
    "### 5. Manual Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02455fe8",
   "metadata": {},
   "source": [
    "Manual tuning involves manually adjusting hyperparameters based on domain knowledge, intuition, and experimentation. While it is the most straightforward approach, it can be time-consuming and subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f3864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 10\n",
      "Cross-Validation Score: 0.9800000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bhushannimje/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "k = 5\n",
    "\n",
    "# Define a range of values for the regularization parameter (C)\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Initialize an empty list to store the cross-validation scores for each C value\n",
    "cv_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation for each value of C\n",
    "for C in C_values:\n",
    "    # Initialize a logistic regression model with the current value of C\n",
    "    model = LogisticRegression(C=C)\n",
    "    \n",
    "    # Initialize a k-fold cross-validation object\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform k-fold cross-validation and compute the mean accuracy\n",
    "    scores = cross_val_score(model, X, y, cv=kf)\n",
    "    mean_score = np.mean(scores)\n",
    "    \n",
    "    # Append the mean accuracy to the list of cross-validation scores\n",
    "    cv_scores.append(mean_score)\n",
    "\n",
    "# Find the index of the C value with the highest cross-validation score\n",
    "best_index = np.argmax(cv_scores)\n",
    "best_C = C_values[best_index]\n",
    "best_score = cv_scores[best_index]\n",
    "\n",
    "# Print the best C value and its corresponding cross-validation score\n",
    "print(f\"Best C: {best_C}\")\n",
    "print(f\"Cross-Validation Score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb955f",
   "metadata": {},
   "source": [
    "#### Code Explanation: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ff241",
   "metadata": {},
   "source": [
    "1. Load Data: We load the iris dataset, which contains features (X) and target labels (y).\n",
    "\n",
    "2. Define Number of Folds: We define the number of folds (k) for k-fold cross-validation.\n",
    "\n",
    "3. Define Range of C Values: We specify a range of values for the regularization parameter C that we want to explore. These values are based on intuition or previous knowledge about the dataset and the model.\n",
    "\n",
    "4. Initialize Empty List: We initialize an empty list (cv_scores) to store the cross-validation scores for each value of C.\n",
    "\n",
    "5. Perform Cross-Validation for Each C Value: We iterate over each value of C in the specified range. For each value, we:\n",
    "\n",
    "    * Initialize a logistic regression model with the current value of C.\n",
    "    * Initialize a k-fold cross-validation object.\n",
    "    * Perform k-fold cross-validation on the model and compute the mean accuracy.\n",
    "    * Append the mean accuracy to the cv_scores list.\n",
    "6. Find Best C Value: We find the index of the C value with the highest cross-validation score (best_index). Then, we retrieve the corresponding C value and its cross-validation score (best_C and best_score, respectively).\n",
    "\n",
    "7. Print Results: We print the best C value and its corresponding cross-validation score.\n",
    "\n",
    "* This example demonstrates how to manually tune hyperparameters using k-fold cross-validation. By evaluating the model's performance across different values of C, we can identify the optimal value that maximizes the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0104f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
