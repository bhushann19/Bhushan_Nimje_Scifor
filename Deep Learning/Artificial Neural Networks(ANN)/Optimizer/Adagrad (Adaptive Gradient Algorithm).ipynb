{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634a0ca1",
   "metadata": {},
   "source": [
    "Adagrad (Adaptive Gradient Algorithm) is an optimization algorithm designed to adapt the learning rate for each parameter based on the gradients' historical magnitudes. This approach allows for different learning rates for each parameter, which can be beneficial for handling sparse data and features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe099cf",
   "metadata": {},
   "source": [
    "### How Adagrad Works\n",
    "Adagrad modifies the learning rate dynamically for each parameter based on the sum of the squares of all past gradients for that parameter. This way, it performs smaller updates for frequently occurring features and larger updates for infrequent features.\n",
    "\n",
    "**1. Initialize:** Start with the initial parameters θ and initialize the sum of squared gradients G to zero.\n",
    "\n",
    "**2. Compute Gradient:** Calculate the gradient gt​ of the loss function at time step t.\n",
    "\n",
    "**3. Update Sum of Squares of Gradients:**  Accumulate the sum of the squares of the gradients. Gt​ = Gt−1​+gt​∘gt​\n",
    "\n",
    "**4. Adjust Learning Rate:** Update the parameters using the accumulated gradient information.\n",
    "\n",
    "θ(t+1) = θt - η/sqrt(Gt+ϵ) ∘ gt​\n",
    " \n",
    " where η is the initial learning rate, Gt​is the sum of the squared gradients up to time step t, ∘ denotes the element-wise multiplication, and ϵ is a small constant to prevent division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b984e0",
   "metadata": {},
   "source": [
    "### Where It Is Used\n",
    "\n",
    "Adagrad is particularly useful in scenarios where:\n",
    "\n",
    "1. The data is sparse.\n",
    "2. Features appear with varying frequencies.\n",
    "3. Handling natural language processing (NLP) tasks, where certain words may be very frequent while others are rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258e69b",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "**1. No Manual Learning Rate Tuning:** Adagrad automatically adapts the learning rate for each parameter, which reduces the need for manual tuning.\n",
    "\n",
    "**2. Works Well with Sparse Data:** The adaptive nature of the algorithm makes it well-suited for sparse data and features, as it performs larger updates for infrequent parameters.\n",
    "\n",
    "**3. Handles Different Feature Frequencies:** By adjusting the learning rate for each parameter, Adagrad can handle features that occur at different frequencies more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b972d5",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "\n",
    "**1. Learning Rate Decay:** One major drawback is that the learning rate can become extremely small over time due to the accumulation of squared gradients, which can cause the training process to stop prematurely.\n",
    "\n",
    "**2. Memory Usage:** Adagrad requires maintaining a sum of squared gradients for each parameter, which can increase memory usage for large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95406720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
