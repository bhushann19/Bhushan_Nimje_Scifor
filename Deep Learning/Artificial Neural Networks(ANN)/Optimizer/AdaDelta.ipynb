{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b900d16",
   "metadata": {},
   "source": [
    "AdaDelta is an extension of the Adagrad optimization algorithm that addresses its aggressive, monotonically decreasing learning rate. It aims to improve the learning rate adaptation by restricting the window of accumulated past gradients to a fixed size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08b175",
   "metadata": {},
   "source": [
    "### How It Works\n",
    "\n",
    "AdaDelta modifies the Adagrad algorithm by replacing the sum of squared gradients with a decaying average of past squared gradients. This helps to reduce the learning rate over time without requiring manual tuning of a global learning rate parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8fea1b",
   "metadata": {},
   "source": [
    "### Where It Is Used\n",
    "\n",
    "AdaDelta is commonly used in training deep neural networks, particularly in scenarios where:\n",
    "\n",
    "1. Adagrad's aggressive learning rate decay becomes problematic.\n",
    "2. A global learning rate parameter is difficult to tune.\n",
    "3. Stable and efficient optimization is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fddda3",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "**1. No Learning Rate Hyperparameter:** AdaDelta eliminates the need for a learning rate hyperparameter, making it easier to implement and less sensitive to hyperparameter tuning.\n",
    "\n",
    "**2. Stable Learning Rates:** AdaDelta dynamically adjusts the learning rates based on the magnitude of recent gradients, leading to more stable convergence.\n",
    "\n",
    "**3. Robust to Noisy Gradients:** AdaDelta's decaying averages help smooth out noisy gradients, making it suitable for training with noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74fdd9",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "\n",
    "**1. Computationally Expensive:**  AdaDelta requires additional computations to maintain the decaying averages of past gradients and parameter updates, which can increase computational cost.\n",
    "\n",
    "**2. Requires Careful Tuning:** While AdaDelta reduces the need for hyperparameter tuning compared to other optimization algorithms, the choice of the decay rate γ and the small constant ϵ still requires careful tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360b27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
