{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0af91f",
   "metadata": {},
   "source": [
    "## What is an Activation Function?\n",
    "\n",
    "An activation function in a neural network introduces non-linearity into the model. This non-linearity allows the network to learn and represent more complex patterns in the data. Activation functions take the weighted sum of inputs and biases and apply a mathematical transformation to produce the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44392c",
   "metadata": {},
   "source": [
    "## Why are Activation Functions Used in CNNs?\n",
    "\n",
    "**1. Introduce Non-Linearity:**\n",
    "Real-world data is often non-linear. Activation functions enable neural networks to learn and model these non-linearities.\n",
    "\n",
    "**2. Feature Learning:**\n",
    "They help the network to learn complex features and hierarchical patterns in the data.\n",
    "\n",
    "**3. Gradient Propagation:**\n",
    "Activation functions aid in the propagation of gradients during backpropagation, which is essential for learning through gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46577b07",
   "metadata": {},
   "source": [
    "## Types of Activation Functions Used in CNNs\n",
    "\n",
    "**1. ReLU (Rectified Linear Unit):**\n",
    "    \n",
    "* Function: $\\operatorname{ReLU}(x)=\\max (0, x)$\n",
    "    \n",
    "* Usage: ReLU is the most commonly used activation function in CNNs.\n",
    "* Advantages:\n",
    "    * Computationally efficient (simple thresholding).\n",
    "    * Helps mitigate the vanishing gradient problem.\n",
    "* Disadvantages:\n",
    "    * Can suffer from the \"dying ReLU\" problem, where neurons can become inactive and stop learning if the inputs are always negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf453d",
   "metadata": {},
   "source": [
    "**2. Leaky ReLU:** \n",
    "\n",
    "* Function: $\\begin{cases}0.01 x & \\text { if } x<0 \\\\ x & \\text { if } x \\geq 0\\end{cases}$\n",
    "* Usage: Addresses the dying ReLU problem by allowing a small, non-zero gradient when the input is negative.\n",
    "* Advantages:\n",
    "    * Prevents neurons from becoming inactive.\n",
    "* Disadvantages:\n",
    "    * Still linear for positive values, which might not capture complex patterns as effectively as other non-linear functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3caf51",
   "metadata": {},
   "source": [
    "**3. Parametric ReLU (PReLU):**\n",
    "\n",
    "* Function: $\\begin{cases}\\alpha x & \\text { if } x<0 \\\\ x & \\text { if } x \\geq 0\\end{cases}$\n",
    "    Here, α is a learnable parameter.\n",
    "* Usage: Similar to Leaky ReLU but with a trainable parameter for negative values.\n",
    "* Advantages:\n",
    "    * Allows the model to learn the optimal value of α.\n",
    "* Disadvantages:\n",
    "    * Adds additional parameters to the model, increasing complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7bc59",
   "metadata": {},
   "source": [
    "**4. Sigmoid:**\n",
    "\n",
    "* Function: $\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
    "* Usage:Previously common in early neural networks; now less frequent in hidden layers of CNNs but used in output layers for binary classification.\n",
    "* Advantages:\n",
    "    * Outputs values in the range (0, 1), useful for probability estimation.\n",
    "* Disadvantages:\n",
    "    * Can suffer from vanishing gradients.\n",
    "    * Computationally expensive compared to ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc44a2e3",
   "metadata": {},
   "source": [
    "**5. Tanh (Hyperbolic Tangent):** \n",
    "* Function: $\\tanh (x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "* Usage: Used in some networks before the popularity of ReLU.\n",
    "* Advantages:\n",
    "    * Outputs values in the range (-1, 1), zero-centered which can help with the optimization.\n",
    "* Disadvantages:\n",
    "    * Similar to sigmoid, it can suffer from vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55551ce",
   "metadata": {},
   "source": [
    "**6. Softmax:**\n",
    "\n",
    "* Function: $\\operatorname{Softmax}\\left(x_i\\right)=\\frac{e^{x_i}}{\\sum_{j=1}^N e^{x_j}}$\n",
    "Converts logits into probabilities.\n",
    "* Usage: Commonly used in the output layer for multi-class classification tasks.\n",
    "* Advantages:\n",
    "    * Provides a probability distribution over classes.\n",
    "* Disadvantages:\n",
    "    * Not used in hidden layers due to computational complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c5f7c",
   "metadata": {},
   "source": [
    "## How Activation Functions are Used in CNNs\n",
    "\n",
    "**1. Applied After Convolution:**\n",
    "After the convolution operation, the activation function is applied to introduce non-linearity.\n",
    "\n",
    "**2. Layer-Wise Integration:**\n",
    "Each convolutional and fully connected layer typically follows a pattern of convolution, activation, and pooling.\n",
    "\n",
    "**3. Gradient-Based Learning:**\n",
    "During backpropagation, the gradient of the loss function with respect to the activation function is computed, which helps in adjusting the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407805e",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Activation Functions\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "**1. Non-Linearity:**\n",
    "Allows the network to model complex, non-linear relationships in data.\n",
    "\n",
    "**2. Gradient Propagation:**\n",
    "Facilitates the propagation of gradients for learning.\n",
    "\n",
    "**3. Efficiency:**\n",
    "Functions like ReLU are computationally simple and efficient to compute.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "**1. Gradient Issues:**\n",
    "Some functions, like sigmoid and tanh, can suffer from vanishing gradients, hindering learning.\n",
    "\n",
    "**2. Dead Neurons:**\n",
    "ReLU can lead to dead neurons where neurons stop learning entirely if they consistently output zero.\n",
    "\n",
    "**3. Parameter Complexity:**\n",
    "Functions like PReLU introduce additional parameters, increasing model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8ed6c",
   "metadata": {},
   "source": [
    "## What is Non-Linearity in CNN?\n",
    "\n",
    "Non-linearity in Convolutional Neural Networks (CNNs) refers to the introduction of non-linear transformations to the input data at various stages of the network. This is achieved through the use of activation functions after convolutional layers. Non-linear activation functions enable the network to learn complex patterns and relationships within the data, which cannot be captured by purely linear operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9914c9",
   "metadata": {},
   "source": [
    "### Why is Non-Linearity Important in CNNs?\n",
    "\n",
    "**1. Complex Pattern Recognition:**\n",
    "Real-world data is inherently non-linear. Non-linear transformations allow CNNs to model and learn these complex patterns.\n",
    "\n",
    "**2. Hierarchical Feature Learning:**\n",
    "Non-linearity enables the network to build hierarchical representations of the input data, with each layer learning increasingly abstract features.\n",
    "\n",
    "**3. Increased Model Capacity:**\n",
    "It enhances the expressive power of the network, allowing it to approximate any continuous function given enough layers and neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545576a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
