{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "101f1347-da47-4b05-ac6e-ab5674e7539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import necessory libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dbbc24-0c7c-4d81-aa0c-d55afc5113aa",
   "metadata": {},
   "source": [
    "# Create model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0bd17eb-a7f1-48ea-83df-94c7dfa4a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    return tf.keras.models.Sequential([  # Define a sequential model\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),  # Flatten 28x28 images to a 1D array\n",
    "        tf.keras.layers.Dense(512, activation='relu'),  # Add a dense layer with 512 neurons and ReLU activation\n",
    "        tf.keras.layers.Dropout(0.2),  # Add dropout layer to prevent overfitting\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # Output layer with 10 neurons (one for each class), using softmax activation\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4973306-09b4-43bd-844b-487d49c4d0ae",
   "metadata": {},
   "source": [
    "# Setup summary writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "923846df-cdeb-42b9-9777-8b41127d3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  # Get current time for logging\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'  # Directory for training logs\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'  # Directory for testing logs\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)  # Create writer for training logs\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)  # Create writer for testing logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b58eb163-e75b-40b9-9fc2-d8d919ecdd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb0b0e0-2dcb-4109-86aa-85f5d9f56e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnist = tf.keras.datasets.mnist  # Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()  # Split into train and test data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize the pixel values to [0, 1]\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).batch(64)  # Create a batched and shuffled dataset for training\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)  # Create a batched dataset for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9fbcb-247b-4655-9e7b-16d8493f76d9",
   "metadata": {},
   "source": [
    "# Define loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b35ba61-3ff8-4899-bf4f-dcc0aabd876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()  # Define the loss function (sparse categorical crossentropy)\n",
    "train_loss = tf.keras.metrics.Mean()  # Track the mean loss for training\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()  # Track accuracy for training\n",
    "test_loss = tf.keras.metrics.Mean()  # Track the mean loss for testing\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()  # Track accuracy for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "479ecdad-28d3-4696-9b85-e0445fb0f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and testing steps\n",
    "optimizer = tf.keras.optimizers.Adam()  # Define the optimizer (Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2f019-47b1-45a3-96b0-f0837f631aca",
   "metadata": {},
   "source": [
    "## Using Custom Training Loops with tf.summary\n",
    "\n",
    "### For more control, you might use tf.GradientTape and tf.summary to log custom metrics.\n",
    "\n",
    "    1. Convert Data to Dataset\n",
    "        Convert data to tf.data.Dataset for efficient batching.\n",
    "\n",
    "## What is tf.GradientTape?\n",
    "### tf.GradientTape is a tool in TensorFlow that helps you calculate how much the output of your model changes when you adjust its inputs or weights. Itâ€™s like a way to see how the model is learning during training.\n",
    "\n",
    "## What is tf.summary?\n",
    "### tf.summary is used to save information about how your model is training. You can think of it as a way to take notes during training so you can look back later and understand how well your model was learning.\n",
    "\n",
    "## Custom Training Loops\n",
    "### When you train a model, TensorFlow usually does everything for you automatically. But sometimes, you want more control to do things your way. For example, you might want to try different ways of updating your model or add custom calculations.\n",
    "\n",
    "## Putting It Together\n",
    "### When you use tf.GradientTape and tf.summary together, you can:\n",
    "\n",
    "#### Manually control how the model learns: You can decide how to update the model step by step, instead of letting TensorFlow do it automatically.\n",
    "\n",
    "#### Log custom metrics: You can save detailed information about how well the model is learning, such as how accurate it is or how much it improves with each step.\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "543af83e-2fb9-44a3-a820-ce6602a8ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function  # Decorator to optimize the training function\n",
    "def train_step(model, x_train, y_train):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation\n",
    "        predictions = model(x_train, training=True)  # Make predictions using the model\n",
    "        loss = loss_object(y_train, predictions)  # Calculate the loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)  # Compute the gradients of the loss w.r.t. model variables\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))  # Update model variables\n",
    "\n",
    "    train_loss(loss)  # Update training loss metric\n",
    "    train_accuracy(y_train, predictions)  # Update training accuracy metric\n",
    "\n",
    "@tf.function  # Decorator to optimize the testing function\n",
    "def test_step(model, x_test, y_test):\n",
    "    predictions = model(x_test)  # Make predictions on the test set\n",
    "    loss = loss_object(y_test, predictions)  # Calculate the loss\n",
    "\n",
    "    test_loss(loss)  # Update testing loss metric\n",
    "    test_accuracy(y_test, predictions)  # Update testing accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15ccfeac-49a8-4ec5-93de-2c9be3f0e550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.24631930887699127, Accuracy: 92.92333221435547, Test Loss: 0.12051034718751907, Test Accuracy: 96.4800033569336\n",
      "Epoch 2, Loss: 0.105633944272995, Accuracy: 96.83999633789062, Test Loss: 0.08637232333421707, Test Accuracy: 97.3699951171875\n",
      "Epoch 3, Loss: 0.07129782438278198, Accuracy: 97.8133316040039, Test Loss: 0.07112293690443039, Test Accuracy: 97.7199935913086\n",
      "Epoch 4, Loss: 0.05508251488208771, Accuracy: 98.25666809082031, Test Loss: 0.06502792984247208, Test Accuracy: 97.82999420166016\n",
      "Epoch 5, Loss: 0.04291757196187973, Accuracy: 98.67832946777344, Test Loss: 0.06042737513780594, Test Accuracy: 98.07999420166016\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "model = create_model()  # Instantiate the model\n",
    "EPOCHS = 5  # Define the number of epochs\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training loop\n",
    "    for (x_train_batch, y_train_batch) in train_dataset:  # Iterate over the training dataset\n",
    "        train_step(model, x_train_batch, y_train_batch)  # Perform a training step\n",
    "    \n",
    "    # Log training metrics\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)  # Log the training loss\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)  # Log the training accuracy\n",
    "\n",
    "    # Testing loop\n",
    "    for (x_test_batch, y_test_batch) in test_dataset:  # Iterate over the testing dataset\n",
    "        test_step(model, x_test_batch, y_test_batch)  # Perform a testing step\n",
    "    \n",
    "    # Log testing metrics\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)  # Log the testing loss\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)  # Log the testing accuracy\n",
    "\n",
    "    # Print metrics for this epoch\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss.result()}, Accuracy: {train_accuracy.result()*100}, Test Loss: {test_loss.result()}, Test Accuracy: {test_accuracy.result()*100}')\n",
    "\n",
    "    # Reset metrics for the next epoch\n",
    "    train_loss.reset_states()  # Reset training loss\n",
    "    test_loss.reset_states()  # Reset testing loss\n",
    "    train_accuracy.reset_states()  # Reset training accuracy\n",
    "    test_accuracy.reset_states()  # Reset testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad218b92-7cfe-4a22-85bb-6e4faa52ab7e",
   "metadata": {},
   "source": [
    "# Check your log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dcddd1-2692-4497-91c7-88b0eaf29d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-16 12:54:36.433827: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-16 12:54:36.551957: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-16 12:54:36.552036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-16 12:54:36.554619: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-16 12:54:36.572671: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-16 12:54:39.332949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.2 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir logs/gradient_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc82c0-92bc-4502-8910-640c69b199a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "669c8aaa-5505-4e47-9a94-f07df7327278",
   "metadata": {},
   "source": [
    "# Delete your Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002aaa0d-539a-4a77-8f79-ef0cb7b15ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Clear previous logs\n",
    "log_dir = 'logs/gradient_tape/'\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "# Continue with the rest of your code...\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = log_dir + current_time + '/train'\n",
    "test_log_dir = log_dir + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93caa77-879d-4d07-aafa-671e4cac50a5",
   "metadata": {},
   "source": [
    "# Manually Delete the Log Directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e55f80f1-d50e-4ef0-87b5-5ba44c2f850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/gradient_tape/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9859e800-0eab-473d-aefe-aa90a53aa5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmdir: failed to remove '/S': No such file or directory\n",
      "rmdir: failed to remove '/Q': No such file or directory\n",
      "rmdir: failed to remove 'logsgradient_tape': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rmdir /S /Q logs\\gradient_tape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab3058-9c7c-4898-a74e-ebb5f6fefd39",
   "metadata": {},
   "source": [
    "# After Deleting Logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad32d24-8d30-421c-925d-143714d399e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=logs/gradient_tape/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398b43e-a9de-4de6-bdb8-416531346a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
